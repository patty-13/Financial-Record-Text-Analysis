{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Change the paths for all the files, \n",
    "#### This work is done by  Pratyush sethi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating file url lists\n",
    "df = pd.read_excel('C:/Users/Praty/OneDrive/Desktop/New folder/cik_list.xlsx')\n",
    "url_list = []\n",
    "for i in range(len(df['SECFNAME'])):\n",
    "    url_list.append(\"https://www.sec.gov/Archives/\" + df['SECFNAME'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13536\n"
     ]
    }
   ],
   "source": [
    "# creating stopping word corpus\n",
    "file_path = ['C:/Users/Praty/OneDrive/Desktop/New folder/StopWords_Auditor.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/StopWords_Currencies.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/StopWords_DatesandNumbers.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/StopWords_Generic.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/StopWords_Geographic.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/StopWords_Names.txt']\n",
    "stopword_corpus = []\n",
    "for i in range(len(file_path)):\n",
    "    with open(file_path[i],encoding=\"mbcs\") as fp:\n",
    "        for line in fp:\n",
    "            line = line.lower()\n",
    "            stopword_corpus.append(line.strip())\n",
    "print(len(stopword_corpus))\n",
    "\n",
    "# Making list of Master dictionary words\n",
    "dict_df = pd.read_csv('C:/Users/Praty/OneDrive/Desktop/New folder/dict.csv')\n",
    "dict_words = [x for x in dict_df['Word'].astype(str)]\n",
    "for i in range(len(dict_words)):\n",
    "    dict_words[i] = dict_words[i].lower()\n",
    "    \n",
    "path_file = ['C:/Users/Praty/OneDrive/Desktop/New folder/positive_words.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/negative_words.txt',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/constraining_dictionary.xlsx',\n",
    "             'C:/Users/Praty/OneDrive/Desktop/New folder/uncertainty_dictionary.xlsx']\n",
    "# making positive word list\n",
    "positive_words = []\n",
    "with open(path_file[0], encoding=\"mbcs\") as fp:\n",
    "        for line in fp:\n",
    "            line = line.lower()\n",
    "            positive_words.append(line.strip())\n",
    "            \n",
    "# making negative word list\n",
    "negative_words = []\n",
    "with open(path_file[1], encoding=\"mbcs\") as fp:\n",
    "        for line in fp:\n",
    "            line = line.lower()\n",
    "            negative_words.append(line.strip())\n",
    "# making uncertainity word list\n",
    "con_dict = pd.read_excel(path_file[2])\n",
    "con_list = [x for x in con_dict['Word']]\n",
    "# making constraining word list\n",
    "un_dict = pd.read_excel(path_file[3])\n",
    "un_list = [x for x in un_dict['Word']]\n",
    "\n",
    "# complex word list\n",
    "complex_word_list = []\n",
    "for i in range(len(dict_df['Word'])):\n",
    "    if dict_df['Syllables'][i] > 2:\n",
    "        complex_word_list.append(dict_df['Word'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "avg_sen_len_1 = []\n",
    "per_of_complex_words = []             \n",
    "fog_index = []\n",
    "complex_word = []\n",
    "word_count = []\n",
    "uncertainty_score = []\n",
    "constraining_score = []\n",
    "positive_word_proportion = []\n",
    "negative_word_proportion = []\n",
    "un_word_proportion = []\n",
    "con_word_proportion = []\n",
    "con_words_whole_report = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 Completed --- 4.009963750839233 seconds ---\n",
      "147 Completed --- 129.4397416114807 seconds ---\n",
      "148 Completed --- 5.168937921524048 seconds ---\n",
      "149 Completed --- 26.16189742088318 seconds ---\n",
      "150 Completed --- 120.62368988990784 seconds ---\n",
      "151 Completed --- 44.74918222427368 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def tokenizing_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = [tok for tok in sentence.split()]\n",
    "    return sentence\n",
    "\n",
    "def open_files(url):\n",
    "    data = []\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    request = Request(url, headers = hdr)\n",
    "    file = urlopen(request)\n",
    "    for line in file:\n",
    "        decoded_line = line.decode(\"utf-8\")\n",
    "        data.append(decoded_line)\n",
    "        \n",
    "    # tokenizing the sentences\n",
    "    processed_sen = [tokenizing_sentence(sentence) for sentence in data]\n",
    "    # number of sentences\n",
    "    num_sen = len(processed_sen)\n",
    "    # removing stopwords and other preprocessing\n",
    "    clean_txt = []\n",
    "    sw_found = 0\n",
    "    for i in range(len(processed_sen)):\n",
    "        for j in range(len(processed_sen[i])):\n",
    "            if not processed_sen[i][j] in stopword_corpus:\n",
    "                clean_txt.append(processed_sen[i][j])\n",
    "            else:\n",
    "                sw_found +=1\n",
    "    # removing html text and integers this is just for dowsizing the words size\n",
    "    cleaner_txt =[]\n",
    "    no_integers = []\n",
    "    clean = re.compile(r'<.*?>')\n",
    "    for i in range(len(clean_txt)):\n",
    "        cleaner_txt.append(re.sub(clean,'',clean_txt[i]))\n",
    "    no_integers = [x for x in cleaner_txt if not (x.isdigit())]  \n",
    "                       \n",
    "    # extracting words by matching them from master dictionary\n",
    "    final_list = []\n",
    "    for i in range(len(no_integers)):\n",
    "        if no_integers[i] in dict_words:\n",
    "            final_list.append(no_integers[i])\n",
    "    # number of words             \n",
    "    total_words = len(final_list)\n",
    "    \n",
    "    # positive words counting                   \n",
    "    positive_count = 0\n",
    "    for i in range(len(final_list)):\n",
    "        if final_list[i] in positive_words:\n",
    "            positive_count +=1                   \n",
    "    # negative word counting\n",
    "    negative_count = 0\n",
    "    for i in range(len(final_list)):\n",
    "        if final_list[i] in negative_words:\n",
    "            negative_count +=1\n",
    "    # complex word count\n",
    "    complex_words_count = 0                   \n",
    "    for i in range(len(final_list)):\n",
    "        if final_list[i] in complex_word_list:\n",
    "            complex_words_count +=1\n",
    "    # constraining words count\n",
    "    con = 0\n",
    "    for i in range(len(final_list)):\n",
    "        if final_list[i] in con_list:\n",
    "            con +=1\n",
    "    # uncertainity words count\n",
    "    un = 0\n",
    "    for i in range(len(final_list)):\n",
    "        if final_list[i] in un_list:\n",
    "            un +=1\n",
    "    # constraining words in whole report\n",
    "    con_words_count = 0\n",
    "    for i in range(len(no_integers)):\n",
    "        if no_integers[i] in con_list:\n",
    "            con_words_count +=1\n",
    "            \n",
    "    # calculating fifteen terms\n",
    "    positive_score.append(positive_count) #1\n",
    "    negative_score.append(negative_count) #2\n",
    "    polarity_score.append((positive_count - negative_count)/(positive_count + negative_count) + 0.000001) #3\n",
    "    avg_sen_len = total_words/len(processed_sen)\n",
    "    avg_sen_len_1.append(total_words /len(processed_sen)) #4\n",
    "    complex_count_word_per = complex_words_count/total_words\n",
    "    per_of_complex_words.append(complex_words_count/ total_words) #5             \n",
    "    fog_index.append(0.4*(avg_sen_len + complex_count_word_per)) #6\n",
    "    complex_word.append(complex_words_count) #7\n",
    "    word_count.append(total_words) #8\n",
    "    uncertainty_score.append(un) #9\n",
    "    constraining_score.append(con) #10\n",
    "    positive_word_proportion.append(positive_count/ total_words) #11\n",
    "    negative_word_proportion.append(negative_count/ total_words) #12\n",
    "    un_word_proportion.append(un/ total_words) #13\n",
    "    con_word_proportion.append(con/ total_words) #14\n",
    "    con_words_whole_report.append(con_words_count) #15\n",
    "    \n",
    "for i in range(146,len(url_list)):\n",
    "    start_time = time.time()\n",
    "    open_files(url_list[i])\n",
    "    print(i,\"Completed\",\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating table\n",
    "table = pd.DataFrame({\"CIK\":df['CIK'],\n",
    "                     \"CONAME\":df['CONAME'],\n",
    "                     \"FYRMO\":df['FYRMO'],\n",
    "                     \"FROM\":df['FDATE'],\n",
    "                     \"SECFNAME\":df['FORM'],\n",
    "                     \"positive_score\":positive_score,\n",
    "                     \"negative_score\":negative_score,\n",
    "                     \"polarity_score\":polarity_score,\n",
    "                     \"average_sentence_length\":avg_sen_len_1,\n",
    "                     \"percentage_of_complex_words\":per_of_complex_words,\n",
    "                     \"fog_index\":fog_index,\n",
    "                     \"complex_word_count\":complex_word,\n",
    "                     \"word_count\":word_count,\n",
    "                     \"uncertainty_score\":uncertainty_score,\n",
    "                     \"constraining_score\":constraining_score,\n",
    "                     \"positive_word_proportion\":positive_word_proportion,\n",
    "                     \"negative_word_proportion\":negative_word_proportion,\n",
    "                     \"uncertainty_word_proportion\":un_word_proportion,\n",
    "                     \"constraining_word_proportion\":con_word_proportion,\n",
    "                     \"constraining_words_Whole_report\":con_words_whole_report})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FROM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>percentage_of_complex_words</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>uncertainty_score</th>\n",
       "      <th>constraining_score</th>\n",
       "      <th>positive_word_proportion</th>\n",
       "      <th>negative_word_proportion</th>\n",
       "      <th>uncertainty_word_proportion</th>\n",
       "      <th>constraining_word_proportion</th>\n",
       "      <th>constraining_words_Whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>3097</td>\n",
       "      <td>1587</td>\n",
       "      <td>0.322375</td>\n",
       "      <td>2.763135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105254</td>\n",
       "      <td>0</td>\n",
       "      <td>63530</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048749</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>2015</td>\n",
       "      <td>864</td>\n",
       "      <td>0.399793</td>\n",
       "      <td>3.036022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.214409</td>\n",
       "      <td>0</td>\n",
       "      <td>42225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.384616</td>\n",
       "      <td>1.502439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600976</td>\n",
       "      <td>0</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029221</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>1277</td>\n",
       "      <td>801</td>\n",
       "      <td>0.229067</td>\n",
       "      <td>2.554894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.021957</td>\n",
       "      <td>0</td>\n",
       "      <td>30881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041352</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-25</td>\n",
       "      <td>10-Q/A</td>\n",
       "      <td>196</td>\n",
       "      <td>227</td>\n",
       "      <td>-0.073285</td>\n",
       "      <td>2.748192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.099277</td>\n",
       "      <td>0</td>\n",
       "      <td>6079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199812</td>\n",
       "      <td>1998-12-22</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>408</td>\n",
       "      <td>418</td>\n",
       "      <td>-0.012106</td>\n",
       "      <td>2.688989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.075596</td>\n",
       "      <td>0</td>\n",
       "      <td>10159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.041146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199812</td>\n",
       "      <td>1998-12-22</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>195</td>\n",
       "      <td>230</td>\n",
       "      <td>-0.082352</td>\n",
       "      <td>2.795918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.118367</td>\n",
       "      <td>0</td>\n",
       "      <td>6302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199903</td>\n",
       "      <td>1999-03-31</td>\n",
       "      <td>NT 10-K</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.428572</td>\n",
       "      <td>1.619910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647964</td>\n",
       "      <td>0</td>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199905</td>\n",
       "      <td>1999-05-11</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1351</td>\n",
       "      <td>855</td>\n",
       "      <td>0.224842</td>\n",
       "      <td>2.920590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.168236</td>\n",
       "      <td>0</td>\n",
       "      <td>31262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043215</td>\n",
       "      <td>0.027349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO       FROM SECFNAME  positive_score  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405            3097   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q            2015   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q               9   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A            1277   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q              12   \n",
       "5  3662  SUNBEAM CORP/FL/  199811 1998-11-25   10-Q/A             196   \n",
       "6  3662  SUNBEAM CORP/FL/  199812 1998-12-22     10-Q             408   \n",
       "7  3662  SUNBEAM CORP/FL/  199812 1998-12-22     10-Q             195   \n",
       "8  3662  SUNBEAM CORP/FL/  199903 1999-03-31  NT 10-K              10   \n",
       "9  3662  SUNBEAM CORP/FL/  199905 1999-05-11     10-K            1351   \n",
       "\n",
       "   negative_score  polarity_score  average_sentence_length  \\\n",
       "0            1587        0.322375                 2.763135   \n",
       "1             864        0.399793                 3.036022   \n",
       "2               4        0.384616                 1.502439   \n",
       "3             801        0.229067                 2.554894   \n",
       "4               4        0.500001                 1.714286   \n",
       "5             227       -0.073285                 2.748192   \n",
       "6             418       -0.012106                 2.688989   \n",
       "7             230       -0.082352                 2.795918   \n",
       "8               4        0.428572                 1.619910   \n",
       "9             855        0.224842                 2.920590   \n",
       "\n",
       "   percentage_of_complex_words  fog_index  complex_word_count  word_count  \\\n",
       "0                          0.0   1.105254                   0       63530   \n",
       "1                          0.0   1.214409                   0       42225   \n",
       "2                          0.0   0.600976                   0         308   \n",
       "3                          0.0   1.021957                   0       30881   \n",
       "4                          0.0   0.685714                   0         384   \n",
       "5                          0.0   1.099277                   0        6079   \n",
       "6                          0.0   1.075596                   0       10159   \n",
       "7                          0.0   1.118367                   0        6302   \n",
       "8                          0.0   0.647964                   0         358   \n",
       "9                          0.0   1.168236                   0       31262   \n",
       "\n",
       "   uncertainty_score  constraining_score  positive_word_proportion  \\\n",
       "0                  0                   0                  0.048749   \n",
       "1                  0                   0                  0.047721   \n",
       "2                  0                   0                  0.029221   \n",
       "3                  0                   0                  0.041352   \n",
       "4                  0                   0                  0.031250   \n",
       "5                  0                   0                  0.032242   \n",
       "6                  0                   0                  0.040161   \n",
       "7                  0                   0                  0.030943   \n",
       "8                  0                   0                  0.027933   \n",
       "9                  0                   0                  0.043215   \n",
       "\n",
       "   negative_word_proportion  uncertainty_word_proportion  \\\n",
       "0                  0.024980                          0.0   \n",
       "1                  0.020462                          0.0   \n",
       "2                  0.012987                          0.0   \n",
       "3                  0.025938                          0.0   \n",
       "4                  0.010417                          0.0   \n",
       "5                  0.037342                          0.0   \n",
       "6                  0.041146                          0.0   \n",
       "7                  0.036496                          0.0   \n",
       "8                  0.011173                          0.0   \n",
       "9                  0.027349                          0.0   \n",
       "\n",
       "   constraining_word_proportion  constraining_words_Whole_report  \n",
       "0                           0.0                                0  \n",
       "1                           0.0                                0  \n",
       "2                           0.0                                0  \n",
       "3                           0.0                                0  \n",
       "4                           0.0                                0  \n",
       "5                           0.0                                0  \n",
       "6                           0.0                                0  \n",
       "7                           0.0                                0  \n",
       "8                           0.0                                0  \n",
       "9                           0.0                                0  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_csv('C:/Users/Praty/OneDrive/Desktop/New folder/output_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_score))\n",
    "print(len(negative_score))\n",
    "print(len(polarity_score))\n",
    "print(len(avg_sen_len_1))\n",
    "print(len(per_of_complex_words))             \n",
    "print(len(fog_index))\n",
    "print(len(complex_word))\n",
    "print(len(word_count))\n",
    "print(len(uncertainty_score))\n",
    "print(len(constraining_score))\n",
    "print(len(positive_word_proportion))\n",
    "print(len(negative_word_proportion))\n",
    "print(len(un_word_proportion))\n",
    "print(len(con_word_proportion))\n",
    "print(len(con_words_whole_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
